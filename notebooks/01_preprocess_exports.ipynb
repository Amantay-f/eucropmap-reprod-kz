{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1 — Preprocess exported GEE CSVs\n",
        "\n",
        "**Input:** multiple CSV shards exported from Google Earth Engine (Sentinel-1 10-day VV/VH composites sampled on polygons).  \n",
        "**Output:** one merged training table in `data/prepared/` with:\n",
        "\n",
        "- `POINT_ID`, `stratum`, `level_1`, `level_2`\n",
        "- feature columns like `VV_YYYYMMDD`, `VH_YYYYMMDD` (optionally `R_YYYYMMDD`)\n",
        "\n",
        "Run this after finishing the GEE export tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def find_repo_root(start: Path | None = None) -> Path:\n",
        "    \"\"\"\n",
        "    Find repository root by looking for README.md and notebooks/ folder.\n",
        "    Works in local Jupyter + Colab after git clone.\n",
        "    \"\"\"\n",
        "    p = (start or Path.cwd()).resolve()\n",
        "    for _ in range(8):\n",
        "        if (p / \"README.md\").exists() and (p / \"notebooks\").exists():\n",
        "            return p\n",
        "        if p.parent == p:\n",
        "            break\n",
        "        p = p.parent\n",
        "    return (start or Path.cwd()).resolve()\n",
        "\n",
        "REPO_ROOT = find_repo_root()\n",
        "\n",
        "DATA_RAW = REPO_ROOT / \"data\" / \"raw\"\n",
        "DATA_PREP = REPO_ROOT / \"data\" / \"prepared\"\n",
        "\n",
        "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
        "DATA_PREP.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"REPO_ROOT:\", REPO_ROOT)\n",
        "print(\"DATA_RAW:\", DATA_RAW)\n",
        "print(\"DATA_PREP:\", DATA_PREP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3_71Sr_t2mS",
        "outputId": "610e21ee-79e8-4ad3-c934-45562b589823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8 CSVs in /content/drive/MyDrive/EU_subset\n",
            " • S1_point_all_10d_10m_20180101-20180731_EU_NE1a_ratio-db.csv\n",
            " • S1_point_all_10d_10m_20180101-20180731_EU_NE1b_ratio-db.csv\n",
            " • S1_point_all_10d_10m_20180101-20180731_EU_NE1c_ratio-db.csv\n",
            " • S1_point_all_10d_10m_20180101-20180731_EU_NE1d_ratio-db.csv\n",
            " • S1_point_all_10d_10m_20180101-20180731_EU_NE2_ratio-db.csv\n",
            "Saved: /content/drive/MyDrive/EU_subset/prepared/LUCAS_2018_Copernicus_attributes_cropmap_level1-2_FROM_EXPORTS.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2452112793.py:86: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
            "  lucas_polygons[col] = pd.to_numeric(lucas_polygons[col], errors='ignore')\n"
          ]
        }
      ],
      "source": [
        "# --- 1_Legend_polygons_COPERNICUS_reorganise_trimmed.py ---\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import glob, os, re\n",
        "\n",
        "DATA_PATH = DATA_RAW\n",
        "OUT_PATH = DATA_PREP\n",
        "\n",
        "# Helper: list what we see\n",
        "all_files = sorted(list(DATA_PATH.glob(\"*.csv\")) + list(DATA_PATH.glob(\"*.CSV\")))\n",
        "print(f\"Found {len(all_files)} CSVs in {DATA_PATH}\")\n",
        "for f in all_files[:5]:\n",
        "    print(\" •\", f.name)\n",
        "if not all_files:\n",
        "    raise FileNotFoundError(f\"No CSV files found in {DATA_PATH}\")\n",
        "\n",
        "# 1) Build the metadata table (POINT_ID, LC1, LU1, stratum) from shards\n",
        "usecols = [\"POINT_ID\", \"LC1\", \"LU1\", \"stratum\"]\n",
        "meta_parts = []\n",
        "for f in all_files:\n",
        "    # Read header first to keep it fast and robust\n",
        "    hdr = pd.read_csv(f, nrows=0).columns.tolist()\n",
        "    cols_present = [c for c in usecols if c in hdr]\n",
        "    df = pd.read_csv(f, usecols=cols_present)\n",
        "    meta_parts.append(df)\n",
        "\n",
        "lucas_polygons = (pd.concat(meta_parts, ignore_index=True)\n",
        "                    .drop_duplicates(subset=[\"POINT_ID\"])\n",
        "                    .reset_index(drop=True))\n",
        "\n",
        "# drop non-relevant LC1 classes (same as original)\n",
        "drop_set = ['A30','H11','H12','H21','H22','H23','G21','G30','G11']\n",
        "lucas_polygons = lucas_polygons[~lucas_polygons.LC1.isin(drop_set)].copy()\n",
        "\n",
        "# LU1s recode (same as original)\n",
        "lucas_polygons['LU1s'] = lucas_polygons['LU1'].astype(str)\n",
        "lucas_polygons['LU1s'] = lucas_polygons['LU1s'].replace(['U111','U112','U113'],'U1')\n",
        "lucas_polygons['LU1s'] = lucas_polygons['LU1s'].replace(\n",
        "    ['U120','U420','U415','U361','U362','U350','U370','U330','U150','U411','U412','U140',\n",
        "     'U312','U341','U314','U414','U210','U319','U318','U222','U321','U322','U413','U311',\n",
        "     'U317','U342','U313','U224','U226','U223','U316','U315','U225','U221','U227'], 'U0'\n",
        ")\n",
        "\n",
        "# Legend mapping to level_2 (exactly as before)\n",
        "def repl(s, pat, val): return s.str.replace(pat, val, regex=True)\n",
        "lucas_polygons['LC1_LU1'] = lucas_polygons['LC1'].astype(str) + lucas_polygons['LU1s'].astype(str)\n",
        "lucas_polygons['level_2'] = lucas_polygons['LC1_LU1']\n",
        "\n",
        "for pat, val in [\n",
        "    (r'A[12][0123]U[01]','100'),\n",
        "    (r'B11U[01]','211'),(r'B12U[01]','212'),(r'B13U[01]','213'),\n",
        "    (r'B14U[01]','214'),(r'B15U[01]','215'),(r'B16U[01]','216'),\n",
        "    (r'B17U[01]','217'),(r'B18U[01]','218'),(r'B19U[01]','219'),\n",
        "    (r'B21U[01]','221'),(r'B22U[01]','222'),(r'B23U[01]','223'),\n",
        "    (r'B3[4-7]U[01]','230'),(r'B31U[01]','231'),(r'B32U[01]','232'),(r'B33U[01]','233'),\n",
        "    (r'B4[1-5]U[01]','240'),(r'B5[1-4]U[01]','250'),(r'B55U[01]','500'),\n",
        "    (r'B7[1-7]U[01]','300'),(r'B8[1-4]U[01]','300'),\n",
        "    (r'C[123][0123]U[01]','300'),(r'D[123][0123]U[01]','300'),\n",
        "    (r'E[123][0123]U[1]','500'),\n",
        "    (r'E[1][0123]U[0]','500'),(r'E[2][0123]U[0]','500'),(r'E[3][0123]U[0]','500'),\n",
        "    (r'F40U1','290'),(r'F[123][0]U[01]','600'),(r'F40U0','600'),\n",
        "]:\n",
        "    lucas_polygons['level_2'] = repl(lucas_polygons['level_2'], pat, val)\n",
        "\n",
        "# level_1 from level_2\n",
        "lucas_polygons['level_1'] = lucas_polygons['level_2']\n",
        "lucas_polygons['level_1'] = lucas_polygons['level_1'].str.replace(r'2[1-6][0-9]', '200', regex=True)\n",
        "lucas_polygons['level_1'] = lucas_polygons['level_1'].str.replace(r'2[7][0-9]', '300', regex=True)\n",
        "lucas_polygons['level_1'] = lucas_polygons['level_1'].str.replace(r'2[8][0-9]', '300', regex=True)\n",
        "lucas_polygons['level_1'] = lucas_polygons['level_1'].str.replace(r'290', '200', regex=True)\n",
        "\n",
        "lucas_polygons = lucas_polygons[['POINT_ID','stratum','LC1','LU1','level_1','level_2']].copy()\n",
        "for col in ['level_1','level_2']:\n",
        "    lucas_polygons[col] = pd.to_numeric(lucas_polygons[col], errors='ignore')\n",
        "\n",
        "legend_csv = OUT_PATH / \"LUCAS_2018_Copernicus_attributes_cropmap_level1-2_FROM_EXPORTS.csv\"\n",
        "lucas_polygons.to_csv(legend_csv, index=False)\n",
        "print(\"Saved:\", legend_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGugdEPaxWI6",
        "outputId": "fb0d90ab-4a13-4077-f301-394bb01a2364"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Script1] Total CSVs found: 8\n",
            "[Script1] First 2 files:\n",
            "  - S1_point_all_10d_10m_20180101-20180731_EU_NE1a_ratio-db.csv\n",
            "  - S1_point_all_10d_10m_20180101-20180731_EU_NE1b_ratio-db.csv\n",
            "\n",
            "[Script1] HEAD of FIRST INPUT CSV:\n",
            "          system:index LC1  LU1  POINT_ID  VH_20180101  VH_20180111  VH_20180121  VH_20180131  VH_20180210  VH_20180220  VH_20180302  VH_20180312  VH_20180322  VH_20180401  VH_20180411  VH_20180421  VH_20180501  VH_20180511  VH_20180521  VH_20180531  VH_20180610  VH_20180620  VH_20180630  VH_20180710  VH_20180720  VV_20180101  VV_20180111  VV_20180121  VV_20180131  VV_20180210  VV_20180220  VV_20180302  VV_20180312  VV_20180322  VV_20180401  VV_20180411  VV_20180421  VV_20180501  VV_20180511  VV_20180521  VV_20180531  VV_20180610  VV_20180620  VV_20180630  VV_20180710  VV_20180720  stratum                                   .geo\n",
            "00010000000000001692_0 B11 U111  47242864   -17.729420   -20.325294   -19.684908   -20.850082   -20.764990   -23.271540   -20.451742   -20.227938   -19.756487   -20.489252   -21.283113   -18.679142   -20.950468   -20.246553   -22.351938   -19.427238   -16.239744   -18.503090   -19.283510   -16.825916   -19.760408    -7.128544   -10.763014    -9.315154    -9.501262   -12.443780   -13.111534   -10.080008    -8.426987    -8.271270   -10.904154   -11.538064   -15.297538   -14.691077   -11.272779   -13.191196   -13.326975    -9.991315   -12.012024   -11.717594   -11.879456   -13.265092        1 {\"type\":\"MultiPoint\",\"coordinates\":[]}\n",
            "00010000000000001692_1 B11 U111  47242864   -17.629759   -20.395664   -19.362911   -20.440153   -21.169271   -23.260570   -19.555204   -19.787247   -20.104240   -20.664425   -21.654552   -19.312412   -20.964762   -20.053510   -22.780458   -19.328346   -16.760923   -18.216225   -19.650293   -17.492460   -18.714783    -6.404769   -10.120535    -9.254024    -9.555243   -12.688955   -11.707333    -9.733994    -8.348043    -7.723035   -11.113006   -11.446243   -13.002155   -14.203595   -10.049500   -11.750811   -12.224334    -8.608410   -12.611928   -11.085283   -11.449447   -12.753875        1 {\"type\":\"MultiPoint\",\"coordinates\":[]}\n",
            "00010000000000001755_0 B11 U111  47322804   -16.761300   -16.439291   -19.003990   -20.409580   -22.091795   -21.210240   -19.267342   -21.996761   -21.559746   -19.284864   -21.190016   -19.270689   -22.382658   -19.068653   -20.096722   -19.126003   -17.581076   -18.512780   -18.273836   -16.363073   -16.860542    -7.052411    -6.625041    -9.611906   -11.761396   -13.199286   -12.959171   -11.692156   -12.756923   -11.699750   -12.339533   -15.035546   -16.568722   -17.411484   -14.883723   -14.744152   -14.379078   -15.570848   -14.374341   -12.383735    -9.408920   -11.391636        1 {\"type\":\"MultiPoint\",\"coordinates\":[]}\n",
            "00010000000000001755_1 B11 U111  47322804   -16.949911   -17.447950   -18.359556   -20.564657   -23.349674   -21.620611   -19.328130   -21.422337   -20.559305   -20.520393   -20.528759   -19.933590   -21.260893   -19.774479   -18.577682   -18.931765   -17.532969   -18.835724   -18.725794   -16.013290   -16.251907    -6.863055    -6.168281    -9.865524   -12.343255   -12.687327   -11.409999   -10.484526   -11.515084   -10.041788   -12.917629   -15.525884   -16.051369   -18.421627   -15.666489   -14.994968   -14.695726   -15.396705   -13.609975   -12.813351   -10.927353   -10.113678        1 {\"type\":\"MultiPoint\",\"coordinates\":[]}\n",
            "00010000000000001755_2 B11 U111  47322804   -16.443756   -16.525919   -18.132175   -20.206577   -23.046797   -22.294617   -19.113546   -21.103245   -20.675358   -20.185593   -21.083435   -19.350054   -21.156912   -19.186129   -19.177694   -18.976183   -17.361813   -19.886072   -19.302000   -16.091885   -16.634918    -6.975775    -6.015145    -9.829995   -12.975266   -12.921446   -13.163848   -11.121654   -11.875488   -10.880135   -12.905340   -15.503733   -15.959276   -18.084450   -14.830488   -15.250996   -14.610595   -15.430483   -15.022743   -12.809982   -10.214397   -10.957893        1 {\"type\":\"MultiPoint\",\"coordinates\":[]}\n"
          ]
        }
      ],
      "source": [
        "print(f\"[Script1] Total CSVs found: {len(all_files)}\")\n",
        "if all_files:\n",
        "    print(\"[Script1] First 2 files:\")\n",
        "    for f in all_files[:2]:\n",
        "        print(\"  -\", f.name)\n",
        "    # Peek at the very first CSV\n",
        "    _tmp = pd.read_csv(all_files[0], nrows=5)\n",
        "    print(\"\\n[Script1] HEAD of FIRST INPUT CSV:\")\n",
        "    print(_tmp.head().to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilNrFvYBxWki",
        "outputId": "5236e026-c75d-4abe-bb88-2f254906b478"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Script1] HEAD of legend table BEFORE SAVE:\n",
            " POINT_ID  stratum LC1  LU1  level_1  level_2\n",
            " 47242864        1 B11 U111      200      211\n",
            " 47322804        1 B11 U111      200      211\n",
            " 47602810        1 B11 U111      200      211\n",
            " 47622814        1 B11 U111      200      211\n",
            " 47642818        1 B11 U111      200      211\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n[Script1] HEAD of legend table BEFORE SAVE:\")\n",
        "print(lucas_polygons.head().to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFaRbTHdzOFA",
        "outputId": "e06670f1-c21f-47ce-ae5d-3e59596ebb1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8 CSVs in /content/drive/MyDrive/EU_subset\n",
            " • S1_point_all_10d_10m_20180101-20180731_EU_NE1a_ratio-db.csv\n",
            " • S1_point_all_10d_10m_20180101-20180731_EU_NE1b_ratio-db.csv\n",
            " • S1_point_all_10d_10m_20180101-20180731_EU_NE1c_ratio-db.csv\n",
            " • S1_point_all_10d_10m_20180101-20180731_EU_NE1d_ratio-db.csv\n",
            " • S1_point_all_10d_10m_20180101-20180731_EU_NE2_ratio-db.csv\n",
            "[DBG] pd_lucas has 'stratum'? -> True\n",
            "[DBG] First 5 columns of pd_lucas: ['system:index', 'LC1', 'LU1', 'POINT_ID', 'VH_20180101']\n",
            "[DBG] Columns after merge (first 20): ['system:index', 'LC1', 'LU1', 'POINT_ID', 'VH_20180101', 'VH_20180111', 'VH_20180121', 'VH_20180131', 'VH_20180210', 'VH_20180220', 'VH_20180302', 'VH_20180312', 'VH_20180322', 'VH_20180401', 'VH_20180411', 'VH_20180421', 'VH_20180501', 'VH_20180511', 'VH_20180521', 'VH_20180531']\n",
            "[DBG] Any 'stratum'-like columns: ['stratum_x', 'stratum_y']\n",
            "[DBG] HEAD of MERGED (with clean 'stratum'):\n",
            " POINT_ID  stratum  level_1  level_2\n",
            " 47242864        1      200      211\n",
            " 47242864        1      200      211\n",
            " 47322804        1      200      211\n",
            " 47322804        1      200      211\n",
            " 47322804        1      200      211\n",
            "Saved: /content/drive/MyDrive/EU_subset/prepared/S1_point_all_10d_10m_20180101-20180731_Stratum1_VV-VH.csv Shape: (1743815, 46)\n",
            "Top level_2 classes:\n",
            " level_2\n",
            "300    11237\n",
            "500     9066\n",
            "211     3056\n",
            "213     1469\n",
            "216     1325\n",
            "232      967\n",
            "214      518\n",
            "250      448\n",
            "290      431\n",
            "215      419\n",
            "222      403\n",
            "240      339\n",
            "221      282\n",
            "218      275\n",
            "100      246\n",
            "Name: count, dtype: Int64\n"
          ]
        }
      ],
      "source": [
        "# --- 2_Polygons_extract_GEE_reorganize_trimmed_S1.py ---\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import glob, os, re\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "DATA_PATH = DATA_RAW\n",
        "OUT_PATH = DATA_PREP\n",
        "\n",
        "# Show what we see\n",
        "all_files = sorted(list(DATA_PATH.glob(\"*.csv\")) + list(DATA_PATH.glob(\"*.CSV\")))\n",
        "print(f\"Found {len(all_files)} CSVs in {DATA_PATH}\")\n",
        "for f in all_files[:5]:\n",
        "    print(\" •\", f.name)\n",
        "if not all_files:\n",
        "    raise FileNotFoundError(f\"No CSV files found in {DATA_PATH}\")\n",
        "\n",
        "# 1) Legend table from script 1\n",
        "legend_csv = OUT_PATH / \"LUCAS_2018_Copernicus_attributes_cropmap_level1-2_FROM_EXPORTS.csv\"\n",
        "lucas_polygons = pd.read_csv(legend_csv, dtype={'level_1':'Int64','level_2':'Int64'})\n",
        "\n",
        "# 2) Load S1 shards\n",
        "parts = []\n",
        "for f in all_files:\n",
        "    df = pd.read_csv(f)\n",
        "    # Drop geometry column only if present\n",
        "    if '.geo' in df.columns:\n",
        "        df = df.drop(columns=['.geo'])\n",
        "    parts.append(df)\n",
        "\n",
        "pd_lucas = pd.concat(parts, ignore_index=True)\n",
        "# ==== DEBUG A: does pd_lucas already have 'stratum'? ====\n",
        "print(\"[DBG] pd_lucas has 'stratum'? ->\", 'stratum' in pd_lucas.columns)\n",
        "print(\"[DBG] First 5 columns of pd_lucas:\", pd_lucas.columns[:5].tolist())\n",
        "\n",
        "# 3) Merge with legend (only POINT_IDs that exist in legend)\n",
        "merged = pd.merge(\n",
        "    pd_lucas,\n",
        "    lucas_polygons[['POINT_ID','stratum','level_2','level_1']],\n",
        "    on='POINT_ID',\n",
        "    how='inner'\n",
        ")\n",
        "# ==== DEBUG B: what did the merge produce? ====\n",
        "print(\"[DBG] Columns after merge (first 20):\", merged.columns[:20].tolist())\n",
        "s_cols = [c for c in merged.columns if 'stratum' in c]\n",
        "print(\"[DBG] Any 'stratum'-like columns:\", s_cols)\n",
        "# ==== FIX: make sure we have a clean 'stratum' column ====\n",
        "if 'stratum' not in merged.columns:\n",
        "    if 'stratum_y' in merged.columns:\n",
        "        merged['stratum'] = merged['stratum_y']\n",
        "    elif 'stratum_x' in merged.columns:\n",
        "        merged['stratum'] = merged['stratum_x']\n",
        "\n",
        "# (Optional) drop the extras to avoid confusion\n",
        "for c in ['stratum_x', 'stratum_y']:\n",
        "    if c in merged.columns:\n",
        "        merged = merged.drop(columns=[c])\n",
        "\n",
        "# 4) Keep only complete rows; then optionally keep stratum 1\n",
        "merged = merged.dropna(subset=['level_1','level_2','stratum'])\n",
        "merged = merged[merged['stratum'] == 1].copy()\n",
        "\n",
        "# 5) Stable feature order; supports VV/VH with optional R_\n",
        "band_cols = sorted([c for c in merged.columns if re.match(r'^(VV_|VH_|R_)\\d{8}$', c)])\n",
        "id_cols = [c for c in ['POINT_ID','stratum'] if c in merged.columns]\n",
        "label_cols = [c for c in ['level_1','level_2'] if c in merged.columns]\n",
        "merged = merged[id_cols + label_cols + band_cols]\n",
        "print(\"[DBG] HEAD of MERGED (with clean 'stratum'):\")\n",
        "print(merged[['POINT_ID','stratum','level_1','level_2']].head().to_string(index=False))\n",
        "\n",
        "# 6) Save with a truthful name (Jan–Jul, Stratum1, VV-VH, add -RATIO if added)\n",
        "out_csv = OUT_PATH / \"S1_point_all_10d_10m_20180101-20180731_Stratum1_VV-VH.csv\"\n",
        "merged.to_csv(out_csv, index=False)\n",
        "print(\"Saved:\", out_csv, \"Shape:\", merged.shape)\n",
        "print(\"Top level_2 classes:\\n\", merged[['POINT_ID','level_2']].drop_duplicates()['level_2'].value_counts().head(15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5VCPJ_CzOdr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCd2ILngCiMK",
        "outputId": "c37c8253-9ead-48cd-b0cc-f38c8bb66a7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       POINT_ID  stratum  LC1   LU1  level_1  level_2\n",
            "0      47242864        1  B11  U111      200      211\n",
            "1      47322804        1  B11  U111      200      211\n",
            "2      47602810        1  B11  U111      200      211\n",
            "3      47622814        1  B11  U111      200      211\n",
            "4      47642818        1  B11  U111      200      211\n",
            "...         ...      ...  ...   ...      ...      ...\n",
            "30929  36503184        1  E20  U111      500      500\n",
            "30930  36643156        1  E20  U111      500      500\n",
            "30931  36703150        1  E20  U111      500      500\n",
            "30932  36503170        1  F40  U111      200      290\n",
            "30933  36883144        1  F40  U112      200      290\n",
            "\n",
            "[30934 rows x 6 columns]\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80uLGDgfCXpd",
        "outputId": "28974727-d63c-4520-e8a2-52c3b8ed391b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded dataset with shape (1743815, 46)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45JQCQBekc21",
        "outputId": "a0836d98-395c-4f8a-bc37-86a43b036e9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes in level_1: [200, 500, 300, 600, 100]\n",
            "Classes in level_2: [211, 212, 213, 214, 215, 216, 218, 219, 221, 222, 223, 231, 232, 233, 230, 240, 250, 290]\n",
            "   POINT_ID  stratum  level_1  level_2  VH_20180101  VH_20180111  VH_20180121  \\\n",
            "0  47242864        1      200      211   -17.729420   -20.325294   -19.684908   \n",
            "1  47242864        1      200      211   -17.629759   -20.395664   -19.362911   \n",
            "2  47322804        1      200      211   -16.761300   -16.439291   -19.003990   \n",
            "3  47322804        1      200      211   -16.949911   -17.447950   -18.359556   \n",
            "4  47322804        1      200      211   -16.443756   -16.525919   -18.132175   \n",
            "\n",
            "   VH_20180131  VH_20180210  VH_20180220  ...  VV_20180421  VV_20180501  \\\n",
            "0   -20.850082   -20.764990   -23.271540  ...   -15.297538   -14.691077   \n",
            "1   -20.440153   -21.169271   -23.260570  ...   -13.002155   -14.203595   \n",
            "2   -20.409580   -22.091795   -21.210240  ...   -16.568722   -17.411484   \n",
            "3   -20.564657   -23.349674   -21.620611  ...   -16.051369   -18.421627   \n",
            "4   -20.206577   -23.046797   -22.294617  ...   -15.959276   -18.084450   \n",
            "\n",
            "   VV_20180511  VV_20180521  VV_20180531  VV_20180610  VV_20180620  \\\n",
            "0   -11.272779   -13.191196   -13.326975    -9.991315   -12.012024   \n",
            "1   -10.049500   -11.750811   -12.224334    -8.608410   -12.611928   \n",
            "2   -14.883723   -14.744152   -14.379078   -15.570848   -14.374341   \n",
            "3   -15.666489   -14.994968   -14.695726   -15.396705   -13.609975   \n",
            "4   -14.830488   -15.250996   -14.610595   -15.430483   -15.022743   \n",
            "\n",
            "   VV_20180630  VV_20180710  VV_20180720  \n",
            "0   -11.717594   -11.879456   -13.265092  \n",
            "1   -11.085283   -11.449447   -12.753875  \n",
            "2   -12.383735    -9.408920   -11.391636  \n",
            "3   -12.813351   -10.927353   -10.113678  \n",
            "4   -12.809982   -10.214397   -10.957893  \n",
            "\n",
            "[5 rows x 46 columns]\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Ju3TKumVXF",
        "outputId": "806b84a6-ea6f-46ed-bbe6-94e87585f0f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data after filtering to crop classes: (604610, 47)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_UJVlOlxqpq",
        "outputId": "d3acc770-2b7f-41cd-dde2-f6682fdfeabc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1]\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c47C6Lk4xt9L",
        "outputId": "765dbee5-e274-421b-dd35-619cfa64a60f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features matrix X shape: (604610, 42)\n",
            "Selected target vector y shape: (604610,)\n",
            "Class distribution in y:\n",
            "Classif\n",
            "211    195821\n",
            "213     89215\n",
            "216     69089\n",
            "232     53663\n",
            "214     31153\n",
            "290     24290\n",
            "215     24001\n",
            "250     23366\n",
            "222     22901\n",
            "240     18351\n",
            "218     16124\n",
            "221     14620\n",
            "230      6360\n",
            "212      4711\n",
            "223      3780\n",
            "219      3446\n",
            "231      1975\n",
            "233      1744\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgJpTe18ydy3",
        "outputId": "19e1db97-7ac5-46e1-d050-05cc1bbc7584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Random Forest trained.\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "qpXoMtxnLBeZ",
        "outputId": "c20f1c79-66a3-460d-8e61-8e9e28d7d9ce"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'baseline_rf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3972949398.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 4) save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 5) save lightweight metadata next to it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'baseline_rf' is not defined"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "l77T4AyBWIXs",
        "outputId": "b3ab823c-1c05-483c-a062-ce3932548933"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Pipeline' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-891853858.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set up a pipeline for RandomizedSearchCV (though scaling isn't needed for RF, pipeline allows consistent param naming)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RFclf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define the hyperparameter search space for the Random Forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# param_dist = {\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A1itTDfWNB5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
